{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D Convolutional Networks for Phoneme Recognition\n",
    "Architecture inspired by https://arxiv.org/pdf/1707.01836.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from speech2phone.preprocessing.TIMIT.phones import get_data, get_phones, phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(data, y):\n",
    "    \"\"\"Resample audio to 800 points.\"\"\"\n",
    "    return signal.resample(data, 800), y\n",
    "\n",
    "def resample512(data, y):\n",
    "    return signal.resample(data, 512), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/pccfs/backed_up/jaredtn/speech2phone/experiments\n",
      "/mnt/pccfs/backed_up/jaredtn/speech2phone/TIMIT/TIMIT\n",
      "Loading train/resample/500 set from cache... done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.path.abspath('../TIMIT/TIMIT'))\n",
    "\n",
    "audio, labels = get_data(preprocessor=resample, batch_preprocess=False, TIMIT_root='../TIMIT/TIMIT', padding=500)\n",
    "phonemes = get_phones(labels)\n",
    "n_phones = len(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([132810, 1, 800])\n",
      "torch.Size([132810])\n"
     ]
    }
   ],
   "source": [
    "audio_tensor = torch.Tensor(audio).unsqueeze(1)\n",
    "labels_tensor = torch.Tensor(labels)\n",
    "print(audio_tensor.shape)\n",
    "print(labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = TensorDataset(audio_tensor, labels_tensor) # Dataset requires same batch dimension\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_embedding: 1600\n",
      "Batches per Epoch: 117\n",
      "[1,   116] train loss: 2.091 test loss: 1.494 train acc 0.505 test acc: 0.519\n",
      "[2,   116] train loss: 1.393 test loss: 1.216 train acc 0.588 test acc: 0.610\n",
      "[3,   116] train loss: 1.173 test loss: 1.088 train acc 0.657 test acc: 0.637\n",
      "[4,   116] train loss: 1.052 test loss: 1.026 train acc 0.680 test acc: 0.653\n",
      "[5,   116] train loss: 0.978 test loss: 0.970 train acc 0.689 test acc: 0.665\n",
      "[6,   116] train loss: 0.924 test loss: 0.968 train acc 0.706 test acc: 0.678\n",
      "[7,   116] train loss: 0.880 test loss: 0.926 train acc 0.688 test acc: 0.688\n",
      "[8,   116] train loss: 0.846 test loss: 0.921 train acc 0.719 test acc: 0.696\n",
      "[9,   116] train loss: 0.814 test loss: 0.876 train acc 0.742 test acc: 0.702\n",
      "[10,   116] train loss: 0.789 test loss: 0.868 train acc 0.737 test acc: 0.709\n",
      "[11,   116] train loss: 0.765 test loss: 0.885 train acc 0.747 test acc: 0.704\n",
      "[12,   116] train loss: 0.744 test loss: 0.899 train acc 0.769 test acc: 0.710\n",
      "[13,   116] train loss: 0.725 test loss: 0.856 train acc 0.758 test acc: 0.720\n",
      "[14,   116] train loss: 0.706 test loss: 0.869 train acc 0.769 test acc: 0.718\n",
      "[15,   116] train loss: 0.693 test loss: 0.906 train acc 0.774 test acc: 0.717\n",
      "[16,   116] train loss: 0.678 test loss: 0.887 train acc 0.765 test acc: 0.730\n",
      "[17,   116] train loss: 0.661 test loss: 0.892 train acc 0.784 test acc: 0.704\n",
      "[18,   116] train loss: 0.648 test loss: 0.894 train acc 0.793 test acc: 0.714\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "[19,   116] train loss: 0.638 test loss: 0.939 train acc 0.777 test acc: 0.713\n",
      "[20,   116] train loss: 0.560 test loss: 0.811 train acc 0.808 test acc: 0.736\n",
      "[21,   116] train loss: 0.532 test loss: 0.804 train acc 0.814 test acc: 0.747\n",
      "[22,   116] train loss: 0.524 test loss: 0.818 train acc 0.816 test acc: 0.746\n",
      "[23,   116] train loss: 0.516 test loss: 0.812 train acc 0.813 test acc: 0.748\n",
      "[24,   116] train loss: 0.508 test loss: 0.819 train acc 0.819 test acc: 0.756\n",
      "[25,   116] train loss: 0.506 test loss: 0.802 train acc 0.830 test acc: 0.753\n",
      "[26,   116] train loss: 0.501 test loss: 0.818 train acc 0.825 test acc: 0.740\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "[27,   116] train loss: 0.497 test loss: 0.819 train acc 0.832 test acc: 0.749\n",
      "[28,   116] train loss: 0.480 test loss: 0.785 train acc 0.834 test acc: 0.754\n",
      "[29,   116] train loss: 0.476 test loss: 0.779 train acc 0.834 test acc: 0.757\n",
      "[30,   116] train loss: 0.475 test loss: 0.826 train acc 0.822 test acc: 0.749\n",
      "[31,   116] train loss: 0.474 test loss: 0.813 train acc 0.825 test acc: 0.756\n",
      "[32,   116] train loss: 0.473 test loss: 0.782 train acc 0.839 test acc: 0.762\n",
      "[33,   116] train loss: 0.472 test loss: 0.806 train acc 0.839 test acc: 0.754\n",
      "[34,   116] train loss: 0.472 test loss: 0.806 train acc 0.840 test acc: 0.746\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[35,   116] train loss: 0.470 test loss: 0.806 train acc 0.851 test acc: 0.745\n",
      "[36,   116] train loss: 0.468 test loss: 0.822 train acc 0.828 test acc: 0.742\n",
      "[37,   116] train loss: 0.469 test loss: 0.819 train acc 0.842 test acc: 0.759\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "[38,   116] train loss: 0.469 test loss: 0.806 train acc 0.831 test acc: 0.762\n"
     ]
    }
   ],
   "source": [
    "class ConvNet1D(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, kernel_size=15, stride=1, padding=7, conv_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.n_channels = 64\n",
    "        self.n_embedding = self.n_channels * 25\n",
    "        print(\"n_embedding: {}\".format(self.n_embedding))\n",
    "        \n",
    "        # input (N, 1, 800) - no initial dropout before residual connections\n",
    "        self.init = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=self.n_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm1d(self.n_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.pool4 = nn.AvgPool1d(kernel_size=4)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "        # (N, 128, 800)\n",
    "        self.downsample4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.n_channels, out_channels=self.n_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm1d(self.n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout),\n",
    "            nn.Conv1d(in_channels=self.n_channels, out_channels=self.n_channels, kernel_size=kernel_size, stride=4, padding=padding)\n",
    "        )\n",
    "        \n",
    "        # (N, 128, 200)\n",
    "        \n",
    "        # apply lots of pooling\n",
    "        \n",
    "        # (N, 128, 25)\n",
    "    \n",
    "        n_res_blocks = 3\n",
    "        channel_lengths = [\n",
    "            self.n_channels * (2 ** (i // 4))\n",
    "            for i in range(n_res_blocks+1)\n",
    "        ]\n",
    "        channel_lengths = [self.n_channels for i in range(n_res_blocks+1)]\n",
    "#         print(channel_lengths)\n",
    "        self.convs = nn.ModuleList([nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_lengths[i]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout),\n",
    "            nn.Conv1d(in_channels=channel_lengths[i], out_channels=channel_lengths[i], kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            \n",
    "            nn.BatchNorm1d(channel_lengths[i]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout),\n",
    "            nn.Conv1d(in_channels=channel_lengths[i], out_channels=channel_lengths[i+1], kernel_size=kernel_size, stride=2, padding=padding),\n",
    "        ) for i in range(n_res_blocks)])\n",
    "        \n",
    "        # (N, 128, 25)\n",
    "        self.predense = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_lengths[-1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense = nn.Linear(self.n_embedding, num_outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.init(x)\n",
    "        x = self.pool4(x) + self.downsample4(x)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = self.pool2(x) + conv(x)\n",
    "#             print(x.shape)\n",
    "                  \n",
    "        x = self.predense(x)\n",
    "        x = x.view(-1, self.n_embedding)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "model = ConvNet1D(num_inputs=800, num_outputs=61).cuda()\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, verbose=True)\n",
    "n_epochs = 50\n",
    "n_print_every = len(train_loader) - 1\n",
    "\n",
    "print(\"Batches per Epoch: {}\".format(len(train_loader)))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, y_truth = data\n",
    "        inputs, y_truth = inputs.cuda(), y_truth.long().cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "#         print(\"inputs: {}, outputs: {}, labels: {}\".format(inputs.shape, outputs.shape, y_truth.shape))\n",
    "        train_loss = criterion(outputs, y_truth)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = torch.argmax(outputs, dim=1)\n",
    "        train_acc = (y_pred == y_truth).float().mean()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += train_loss.item()\n",
    "        if i % n_print_every == n_print_every - 1:    # print every 2000 mini-batches\n",
    "            inputs, y_truth = next(iter(test_loader))\n",
    "            inputs, y_truth = inputs.cuda(), y_truth.long().cuda()\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, y_truth)\n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            test_acc = (y_pred == y_truth).float().mean()\n",
    "            scheduler.step(test_acc) # anneal the learning rate if no progress for 5 epochs\n",
    "            \n",
    "            print('[%d, %5d] train loss: %.3f test loss: %.3f train acc %.3f test acc: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / n_print_every, test_loss, train_acc, test_acc))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "                            \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results so far: 66.5% after 10 epochs (no dropout, 5 128-channel kernel-size-4 convolutional layers, 4-kernel avgpool).  \n",
    "Increasing the number of convolution channels from 64 to 128 works quite well, with limited signs of overfitting.\n",
    "Increasing the learning rate from 1e-4 to 1e-3 boosted things as well (66.2% after 5 epochs).  \n",
    "I'm experimenting with skip connections; hopefully they'll help!  \n",
    "Dropout just makes things worse.  \n",
    "model.eval() makes things worse. That's because of how it interacts with the BatchNorm layer; it uses the stored mean/var instead of computing batch statistics.  \n",
    "\n",
    "Better results!!! 67.7% after 10 epochs (dropout 0.5, 5 layers of 64-channel 16-size-kernel convolutional layers).  \n",
    "Removing residual connections gave 68.1% after 10 epochs. Probably just noise though.  \n",
    "\n",
    "Copying the \"Cardiologist-Level Arrhythmia Detection\" provided 70.0% accuracy after 10 epochs, with dropout 0.5 and Adam lr 3e-3, 64-channel 16-size-kernel convolutional layers.  \n",
    "Using ReduceLROnPlateau with patience 2 also helped, as did reducing conv_dropout from 0.5 to 0.2, as in their paper. 70.7% accuracy after 10.\n",
    "Bumping the Adam learning rate up from 1e-3 to 1e-2, combined with ReduceLROnPlateau, gave us 73.0% accuracy after 10 epochs.  \n",
    "\n",
    "Large batch sizes work well. batch size 32 converges at 67.7%, batch size 128 converges at 73.0%, batch size 256 converges at 75.7%, batch size 512 converges at 73.8%, batch size 1024 converges at 75.9%.  \n",
    "Batch size 512 converges significantly slower, taking 12 epochs to cross 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([p.numel() for p in model.parameters()]) # Number of parameters in each layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
